{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e6785b-6a5c-487e-8ef0-6fb5ecd0640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eea5737-d891-4076-b7ca-aeaff90fdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Salesforce/codegen-350M-mono\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8c3cdb-8f76-4b65-b43e-8d24998b5c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/VikingHacks/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f62f507-db9e-4a89-9e18-7797a4bb9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b7821f-ec0c-4ad8-b3a9-2469b0ea9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"def factorial():\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895bd5be-96a4-4af0-92b4-99cfc8b9f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8eb8e9-1d8e-4ee8-8910-17f4badcc047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def factorial():\n",
      "    print(\"Factorial\")\n",
      "    n = int(input(\"Enter\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(completion[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d7cefa-aaeb-4544-9d99-07cd9318e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48074ae4-cc0a-43b6-8fce-f61549221972",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "def factorial(x, k):\n",
    "    if (x == 0):\n",
    "        return k(1)\n",
    "    return factorial(x-1, lambda res: k(x * res))\n",
    "\n",
    "Summarize this code.\n",
    "\"\"\"\n",
    "model_name = \"codellama:7b-instruct-q4_K_M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3169b8-7867-498d-a9fb-90e4cf46ed2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code defines a function called `factorial` that takes two arguments: `x`, which is the number to be factored, and `k`, which is a callback function that will be called with the result of the factorial calculation. The function uses recursion to calculate the factorial of `x` by calling itself with `x-1` as the argument until it reaches the base case where `x` is 0, at which point it returns the result of calling the callback function `k` with the value 1.\n",
       "\n",
       "The code also defines a lambda expression for the callback function `k`, which takes one argument `res` and returns the result of multiplying `x` by `res`. This lambda expression is used to define the behavior of the callback function when it is called with the result of the factorial calculation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ],\n",
    "    options={\n",
    "        'temperature': 0.3, # Lower temperature for more factual/less creative summaries\n",
    "        'num_predict': 200, # Max number of tokens to generate for the summary\n",
    "    }\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response['message']['content'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329e91dc-0806-422e-a34d-739d18221a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Code:\n",
       "```\n",
       "def factorial(n):\n",
       "    if n == 0:\n",
       "        return 1\n",
       "    else:\n",
       "        return n * factorial(n-1)\n",
       "```\n",
       "Description:\n",
       "This code defines a function called `factorial` that takes an integer `n` as input. The function calculates the factorial of `n` by recursively calling itself with `n-1` until it reaches 0, at which point it returns 1. The final result is the product of all the factors from `n` down to 1.\n",
       "\n",
       "For example, if we call the function with `n=5`, it will calculate the factorial as follows:\n",
       "```\n",
       "factorial(5) = 5 * factorial(4)\n",
       "              = 5 * (4 * factorial(3))\n",
       "              = 5 * (4 * (3 * factorial(2)))\n",
       "              = 5 *"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt2 = \\\n",
    "\"\"\"\n",
    "Write me Python code that finds factorial of n. Write the code first, and then write the description. Write it like this:\n",
    "Code:\n",
    "  <WRITE CODE>\n",
    "Description:\n",
    "  <WRITE DESCRIPTION>\n",
    "\"\"\"\n",
    "response2 = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt2},\n",
    "    ],\n",
    "    options={\n",
    "        'temperature': 0.3, # Lower temperature for more factual/less creative summaries\n",
    "        'num_predict': 200, # Max number of tokens to generate for the summary\n",
    "    }\n",
    ")\n",
    "\n",
    "#print(response2['message']['content'].strip())\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response2['message']['content'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37469f7-c03f-4e2e-a51f-82144c99d890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Code:\n",
       "```c++\n",
       "#include <iostream>\n",
       "using namespace std;\n",
       "\n",
       "int factorial(int x, int k) {\n",
       "  if (x == 0) {\n",
       "    return k(1);\n",
       "  } else {\n",
       "    return factorial(x-1, [res](int res) {\n",
       "      return k(x * res);\n",
       "    });\n",
       "  }\n",
       "}\n",
       "```\n",
       "Description: This code defines a function called `factorial` that takes two arguments: an integer `x` and a lambda function `k`. The function returns the factorial of `x`, which is calculated recursively by calling itself with `x-1` as the first argument, and `lambda res: k(x * res)` as the second argument. The base case is when `x` is 0, in which case the function returns `k(1)`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt3 = \\\n",
    "\"\"\"\n",
    "def factorial(x, k):\n",
    "    if (x == 0):\n",
    "        return k(1)\n",
    "    return factorial(x-1, lambda res: k(x * res))\n",
    "\n",
    "Translate this code to C++. Return it in this format:\n",
    "\n",
    "Code:\n",
    "    <WRITE CODE>\n",
    "Description:\n",
    "    <WRITE DESCRIPTION>\n",
    "\n",
    "Keep the description concise.\n",
    "\"\"\"\n",
    "response3 = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt3},\n",
    "    ],\n",
    "    options={\n",
    "        'temperature': 0.3, # Lower temperature for more factual/less creative summaries\n",
    "        'num_predict': 200, # Max number of tokens to generate for the summary\n",
    "    }\n",
    ")\n",
    "\n",
    "display(Markdown((response3['message']['content'].strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d96434-eb21-419d-87a5-3cbe004f1ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
